"""
Streamlit + Ollama ã‚·ãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒª
Qwen2.5-7bãƒ¢ãƒ‡ãƒ«ã¨ã®å¯¾è©±ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
"""

import streamlit as st
import ollama
from datetime import datetime
import json
import re

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="Ollama Chat - Qwen2.5",
    page_icon="ğŸ¤–",
    layout="wide"
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if "messages" not in st.session_state:
    st.session_state.messages = []
if "model" not in st.session_state:
    st.session_state.model = "qwen2.5:7b"
if "temperature" not in st.session_state:
    st.session_state.temperature = 0.7

# ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
with st.sidebar:
    st.title("âš™ï¸ è¨­å®š")
    
    # ãƒ¢ãƒ‡ãƒ«é¸æŠ
    try:
        # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
        models = ollama.list()
        model_names = [model['name'] for model in models['models']]
        
        # Qwen2.5-7bãŒã‚ã‚‹ã‹ç¢ºèª
        if "qwen2.5:7b" not in model_names and "qwen2.5:latest" not in model_names:
            st.warning("âš ï¸ Qwen2.5-7bãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            st.code("ollama pull qwen2.5:7b", language="bash")
            # ä»£æ›¿ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
            if model_names:
                st.session_state.model = st.selectbox(
                    "ä»£æ›¿ãƒ¢ãƒ‡ãƒ«é¸æŠ",
                    model_names,
                    index=0
                )
        else:
            st.session_state.model = st.selectbox(
                "ãƒ¢ãƒ‡ãƒ«é¸æŠ",
                model_names,
                index=model_names.index("qwen2.5:7b") if "qwen2.5:7b" in model_names else 0
            )
    except Exception as e:
        st.error(f"Ollamaæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        st.info("OllamaãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        st.code("sudo systemctl status ollama", language="bash")
    
    st.divider()
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    st.session_state.temperature = st.slider(
        "Temperature",
        min_value=0.0,
        max_value=2.0,
        value=st.session_state.temperature,
        step=0.1,
        help="é«˜ã„ã»ã©å‰µé€ çš„ã€ä½ã„ã»ã©ä¸€è²«æ€§ã®ã‚ã‚‹å¿œç­”"
    )
    
    top_p = st.slider(
        "Top P",
        min_value=0.0,
        max_value=1.0,
        value=0.9,
        step=0.05,
        help="ãƒˆãƒ¼ã‚¯ãƒ³é¸æŠã®ç´¯ç©ç¢ºç‡é–¾å€¤"
    )
    
    max_tokens = st.number_input(
        "æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°",
        min_value=50,
        max_value=4096,
        value=1024,
        step=50
    )
    
    st.divider()
    
    # ä¼šè©±å±¥æ­´ç®¡ç†
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ—‘ï¸ å±¥æ­´ã‚¯ãƒªã‚¢", use_container_width=True):
            st.session_state.messages = []
            st.rerun()
    
    with col2:
        # å±¥æ­´ã‚’JSONã§ä¿å­˜
        if st.button("ğŸ’¾ å±¥æ­´ä¿å­˜", use_container_width=True):
            if st.session_state.messages:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"chat_history_{timestamp}.json"
                json_str = json.dumps(st.session_state.messages, ensure_ascii=False, indent=2)
                st.download_button(
                    label="ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=json_str,
                    file_name=filename,
                    mime="application/json"
                )
    
    st.divider()
    
    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
    with st.expander("ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±"):
        st.code(f"""
ãƒ¢ãƒ‡ãƒ«: {st.session_state.model}
Temperature: {st.session_state.temperature}
Top P: {top_p}
æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³: {max_tokens}
ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°: {len(st.session_state.messages)}
        """)

# ãƒ¡ã‚¤ãƒ³ãƒãƒ£ãƒƒãƒˆç”»é¢
st.title("ğŸ¤– Ollama Chat Interface")
st.caption(f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {st.session_state.model}")


def _sanitize_output(text: str) -> str:
    """UIè¡¨ç¤ºç”¨ã«æ€è€ƒéç¨‹ï¼ˆCoTï¼‰ã‚’é™¤å»ã™ã‚‹è»½é‡ãƒ•ã‚£ãƒ«ã‚¿"""
    try:
        s = str(text)
        # ã‚¿ã‚°å½¢å¼
        s = re.sub(r"(?is)<\s*(think|thought|scratchpad)\b[^>]*>.*?<\s*/\s*\1\s*>", "", s)
        s = re.sub(r"(?is)<\s*(think|thought|scratchpad)\b[^>]*>[\s\S]*$", "", s)
        # ãƒ•ã‚§ãƒ³ã‚¹ä»˜ãã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯
        s = re.sub(r"(?is)```\s*(reasoning|thought|think)[\s\S]*?```", "", s)
        s = re.sub(r"(?is)```\s*(reasoning|thought|think)[\s\S]*$", "", s)
        # æ—¥æœ¬èªãƒ©ãƒ™ãƒ«
        s = re.sub(r"(?s)ã€æ€è€ƒã€‘[\s\S]*?ã€/æ€è€ƒã€‘", "", s)
        s = re.sub(r"(?s)ã€æ€è€ƒã€‘[\s\S]*$", "", s)
        # è¡Œé ­ã®ã€æ€è€ƒ:ã€ãªã©
        s = re.sub(r"(?mi)^(æ€è€ƒ|æ¨è«–|è€ƒãˆ|Reasoning|Thoughts?)\s*[:ï¼š].*$\n?", "", s)
        # é€£ç¶šæ”¹è¡Œã®æ•´ç†
        s = re.sub(r"\n{3,}", "\n\n", s).strip()
        return s
    except Exception:
        return text

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤º
chat_container = st.container()
with chat_container:
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
if prompt := st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."):
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”
            with st.spinner("è€ƒãˆä¸­..."):
                # Ollamaã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡
                stream = ollama.chat(
                    model=st.session_state.model,
                    messages=st.session_state.messages,
                    stream=True,
                    options={
                        "temperature": st.session_state.temperature,
                        "top_p": top_p,
                        # å®‰å…¨ãªæ—¢å®š: KVã‚­ãƒ£ãƒƒã‚·ãƒ¥/VRAMåœ§è¿«ã‚’æŠ‘ãˆã‚‹
                        "num_ctx": 4096,
                        "num_batch": 128,
                        "num_predict": max_tokens,
                    }
                )
                
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º
                for chunk in stream:
                    if chunk['message']['content']:
                        full_response += chunk['message']['content']
                        display_text = _sanitize_output(full_response)
                        message_placeholder.markdown(display_text + "â–Œ")
                
                message_placeholder.markdown(_sanitize_output(full_response))
        
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            full_response = "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚å¿œç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
            message_placeholder.markdown(full_response)
    
    # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
    st.session_state.messages.append({"role": "assistant", "content": _sanitize_output(full_response)})

# ãƒ•ãƒƒã‚¿ãƒ¼
st.divider()
col1, col2, col3 = st.columns(3)
with col1:
    st.caption("ğŸš€ Powered by Ollama")
with col2:
    st.caption("ğŸ¯ RTX A5000 Optimized")
with col3:
    st.caption("ğŸ“ Ubuntu 24.04")
