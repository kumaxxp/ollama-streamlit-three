#!/usr/bin/env python3
"""
Ollamaãƒ¢ãƒ‡ãƒ«ç¢ºèªãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Advanced Dialogue Systemç”¨
"""

import sys
import ollama
from typing import List, Set
from app.core.model_utils import ModelManager

def check_ollama_connection() -> bool:
    """Ollamaæ¥ç¶šç¢ºèª"""
    try:
        client = ollama.Client()
        client.list()
        print("âœ… Ollamaã«æ¥ç¶šã§ãã¾ã—ãŸ")
        return True
    except Exception as e:
        print(f"âŒ Ollamaã«æ¥ç¶šã§ãã¾ã›ã‚“: {e}")
        print("\nä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„:")
        print("1. OllamaãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹: https://ollama.ai")
        print("2. OllamaãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹: ollama serve")
        return False

def list_available_models() -> List[str]:
    """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º"""
    manager = ModelManager()
    models = manager.get_available_models()
    
    if not models:
        print("âš ï¸ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
        return []
    
    print(f"\nğŸ“¦ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« ({len(models)}å€‹):")
    print("-" * 50)
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«åˆ†é¡
    categories = {
        "Qwenç³»": [],
        "Gemmaç³»": [],
        "Llamaç³»": [],
        "GPT-OSS": [],
        "DeepSeekç³»": [],
        "ãã®ä»–": []
    }
    
    for model in models:
        if "qwen" in model.lower():
            categories["Qwenç³»"].append(model)
        elif "gemma" in model.lower():
            categories["Gemmaç³»"].append(model)
        elif "llama" in model.lower():
            categories["Llamaç³»"].append(model)
        elif "gpt" in model.lower():
            categories["GPT-OSS"].append(model)
        elif "deepseek" in model.lower():
            categories["DeepSeekç³»"].append(model)
        else:
            categories["ãã®ä»–"].append(model)
    
    for category, model_list in categories.items():
        if model_list:
            print(f"\n{category}:")
            for model in model_list:
                print(f"  â€¢ {model}")
    
    return models

def check_recommended_models(available_models: Set[str]) -> None:
    """æ¨å¥¨ãƒ¢ãƒ‡ãƒ«ã®ç¢ºèª"""
    print("\nğŸ¯ æœ¬ç•ªç’°å¢ƒæ¨å¥¨ãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ…‹:")
    print("-" * 50)
    
    # æœ¬ç•ªç’°å¢ƒã§æ¡ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
    production_models = [
        ("qwen2.5:7b-instruct-q4_K_M", "æ—¥æœ¬èªå¯¾è©±ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ¨å¥¨", "4.7GB", "â˜…â˜…â˜…â˜…â˜…"),
        ("gemma3:12b", "é«˜å“è³ªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ", "8.1GB", "â˜…â˜…â˜…â˜…â˜†"),
        ("gpt-oss:20b", "å‰µé€ çš„å¯¾è©±ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ", "13GB", "â˜…â˜…â˜…â˜†â˜†"),
        ("gemma3:4b", "Directoræ¨å¥¨ï¼ˆé«˜é€Ÿåˆ¤æ–­ï¼‰", "3.3GB", "â˜…â˜…â˜…â˜…â˜†"),
        ("qwen:7b", "ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨", "4.5GB", "â˜…â˜…â˜…â˜†â˜†")
    ]
    
    print(f"{'ãƒ¢ãƒ‡ãƒ«å':<30} {'ç”¨é€”':<25} {'ã‚µã‚¤ã‚º':<8} {'æ¨å¥¨åº¦'}")
    print("-" * 80)
    
    missing = []
    for model_name, description, size, rating in production_models:
        if model_name in available_models:
            print(f"âœ… {model_name:<30} {description:<25} {size:<8} {rating}")
        else:
            print(f"âŒ {model_name:<30} {description:<25} {size:<8} {rating}")
            missing.append(model_name)
    
    if missing:
        print("\nğŸ’¡ æ¨å¥¨ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚³ãƒãƒ³ãƒ‰:")
        print("-" * 50)
        for model in missing:
            print(f"   ollama pull {model}")
        
        print("\nğŸ“Œ ç‰¹ã«é‡è¦:")
        print("   ollama pull qwen2.5:7b-instruct-q4_K_M  # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”¨")
        print("   ollama pull gemma3:4b                   # Directorç”¨")

def suggest_models_by_vram() -> None:
    """VRAMå®¹é‡åˆ¥ã®æ¨å¥¨"""
    print("\nğŸ’» VRAMå®¹é‡åˆ¥ã®æ¨å¥¨ãƒ¢ãƒ‡ãƒ«:")
    print("-" * 50)
    
    suggestions = {
        "3-4GB": ["llama3.2:3b", "phi3:mini"],
        "6-8GB": ["qwen2.5:7b", "gemma2:9b", "llama3.1:8b"],
        "12-16GB": ["qwen2.5:14b", "gemma3:12b", "deepseek-r1:14b"],
        "24GBä»¥ä¸Š": ["qwen2.5:32b", "gemma2:27b", "mixtral:8x7b"]
    }
    
    for vram, models in suggestions.items():
        print(f"\nVRAM {vram}:")
        for model in models:
            print(f"  â€¢ {model}")

def download_model(model_name: str) -> bool:
    """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    print(f"\nğŸ“¥ {model_name} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
    try:
        client = ollama.Client()
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤ºã®ãŸã‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        for progress in client.pull(model_name, stream=True):
            status = progress.get('status', '')
            if 'total' in progress and 'completed' in progress:
                total = progress['total']
                completed = progress['completed']
                percent = (completed / total) * 100 if total > 0 else 0
                print(f"\r{status}: {percent:.1f}%", end='')
            else:
                print(f"\r{status}", end='')
        
        print(f"\nâœ… {model_name} ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸ")
        return True
        
    except Exception as e:
        print(f"\nâŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def interactive_download() -> None:
    """å¯¾è©±çš„ãªãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    print("\nğŸ“¦ æœ¬ç•ªç’°å¢ƒæ¨å¥¨ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã‹ï¼Ÿ")
    print("1. qwen2.5:7b-instruct-q4_K_M (ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ¨å¥¨ã€4.7GB)")
    print("2. gemma3:4b (Directoræ¨å¥¨ã€3.3GB)")
    print("3. gemma3:12b (é«˜å“è³ªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€8.1GB)")
    print("4. gpt-oss:20b (å‰µé€ çš„å¯¾è©±ã€13GB)")
    print("5. å…¨æ¨å¥¨ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
    print("6. ã‚«ã‚¹ã‚¿ãƒ å…¥åŠ›")
    print("0. ã‚¹ã‚­ãƒƒãƒ—")
    
    choice = input("\né¸æŠ (0-6): ").strip()
    
    model_map = {
        "1": "qwen2.5:7b-instruct-q4_K_M",
        "2": "gemma3:4b",
        "3": "gemma3:12b",
        "4": "gpt-oss:20b"
    }
    
    if choice in model_map:
        download_model(model_map[choice])
    elif choice == "5":
        # å…¨æ¨å¥¨ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
        models_to_install = [
            "qwen2.5:7b-instruct-q4_K_M",
            "gemma3:4b",
            "gemma3:12b",
            "gpt-oss:20b",
            "qwen:7b"
        ]
        for model in models_to_install:
            download_model(model)
    elif choice == "6":
        custom_model = input("ãƒ¢ãƒ‡ãƒ«åã‚’å…¥åŠ›: ").strip()
        if custom_model:
            download_model(custom_model)
    elif choice == "0":
        print("ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("ğŸ­ Advanced Dialogue System - ãƒ¢ãƒ‡ãƒ«ç¢ºèªãƒ„ãƒ¼ãƒ«")
    print("=" * 60)
    
    # Ollamaæ¥ç¶šç¢ºèª
    if not check_ollama_connection():
        sys.exit(1)
    
    # ãƒ¢ãƒ‡ãƒ«ä¸€è¦§è¡¨ç¤º
    available = list_available_models()
    available_set = set(available)
    
    # æ¨å¥¨ãƒ¢ãƒ‡ãƒ«ç¢ºèª
    check_recommended_models(available_set)
    
    # VRAMåˆ¥æ¨å¥¨
    suggest_models_by_vram()
    
    # å¯¾è©±çš„ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    if len(available) == 0:
        print("\nâš ï¸ ãƒ¢ãƒ‡ãƒ«ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        interactive_download()
    else:
        choice = input("\nè¿½åŠ ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã‹ï¼Ÿ (y/n): ").strip().lower()
        if choice == 'y':
            interactive_download()
    
    print("\nâœ¨ ç¢ºèªå®Œäº†ï¼")
    print("Streamlitã‚¢ãƒ—ãƒªã‚’èµ·å‹•: streamlit run app/pages/03_Advanced_Dialogue_Refactored.py")

if __name__ == "__main__":
    main()