# ğŸ¤– Ollama Streamlit Chat

ãƒ­ãƒ¼ã‚«ãƒ«LLMã¨å¯¾è©±ã™ã‚‹ãŸã‚ã®é«˜æ©Ÿèƒ½WebUIãƒãƒ£ãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ ã€‚Ollama + Streamlitã§æ§‹ç¯‰ã•ã‚ŒãŸã€æ‹¡å¼µå¯èƒ½ãªAIãƒãƒ£ãƒƒãƒˆãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã™ã€‚

![Python](https://img.shields.io/badge/python-3.11-blue.svg)
![Streamlit](https://img.shields.io/badge/streamlit-1.32.0-red.svg)
![Ollama](https://img.shields.io/badge/ollama-0.3.0-green.svg)
![License](https://img.shields.io/badge/license-MIT-yellow.svg)

## ğŸ“‹ æ¦‚è¦

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å‹•ä½œã™ã‚‹LLMï¼ˆLarge Language Modelï¼‰ã¨å¯¾è©±ã™ã‚‹ãŸã‚ã®Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚Ollamaã‚’ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¨ã—ã¦ä½¿ç”¨ã—ã€Streamlitã§æ§‹ç¯‰ã•ã‚ŒãŸç›´æ„Ÿçš„ãªUIã‚’é€šã˜ã¦ã€æ§˜ã€…ãªAIãƒ¢ãƒ‡ãƒ«ã¨ä¼šè©±ã§ãã¾ã™ã€‚

### ğŸ¯ ä¸»ãªç‰¹å¾´

- **ğŸš€ é«˜é€Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹**: RTX A5000æœ€é©åŒ–ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”å¯¾å¿œ
- **ğŸ¨ è¤‡æ•°ãƒ¢ãƒ‡ãƒ«å¯¾å¿œ**: Qwen2.5ã€Gemma3ã€GPT-OSSç­‰ã€åˆ‡ã‚Šæ›¿ãˆå¯èƒ½
- **ğŸ’¾ ä¼šè©±ç®¡ç†**: å±¥æ­´ä¿å­˜ã€ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã€æ¤œç´¢æ©Ÿèƒ½
- **âš™ï¸ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã€ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®š
- **ğŸ“Š åˆ†ææ©Ÿèƒ½**: ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã€å¿œç­”æ™‚é–“ã€ä¼šè©±ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
- **ğŸ”§ æ‹¡å¼µå¯èƒ½**: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­è¨ˆã€ãƒ—ãƒ©ã‚°ã‚¤ãƒ³å¯¾å¿œ

## ğŸ—ï¸ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

```
ollama-streamlit-chat/
â”œâ”€â”€ app/                    # ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
â”‚   â”œâ”€â”€ simple_chat.py     # åŸºæœ¬ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½
â”‚   â””â”€â”€ pages/             # ãƒãƒ«ãƒãƒšãƒ¼ã‚¸æ©Ÿèƒ½
â”œâ”€â”€ config/                # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
â”‚   â”œâ”€â”€ models.yaml        # ãƒ¢ãƒ‡ãƒ«è¨­å®š
â”‚   â””â”€â”€ characters/        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å®šç¾©
â”œâ”€â”€ utils/                 # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”œâ”€â”€ data/                  # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
â””â”€â”€ scripts/               # ä¾¿åˆ©ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
```

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### å‰ææ¡ä»¶

- Ubuntu 24.04 (æ¨å¥¨) / Windows 11 / macOS
- Python 3.11ä»¥ä¸Š
- Conda (Anaconda/Miniconda)
- Ollama v0.3.0ä»¥ä¸Š
- GPU: NVIDIA RTX A5000 (æ¨å¥¨) / 8GBä»¥ä¸Šã®VRAM

### 1. ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³

```bash
git clone https://github.com/yourusername/ollama-streamlit-chat.git
cd ollama-streamlit-chat
```

### 2. è‡ªå‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆæ¨å¥¨ï¼‰

```bash
chmod +x setup_and_run.sh
./setup_and_run.sh
```

### 3. æ‰‹å‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

#### Condaç’°å¢ƒã®ä½œæˆ

```bash
# ç’°å¢ƒä½œæˆ
conda env create -f environment.yml
conda activate ollama-chat

# ã¾ãŸã¯æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
conda create -n ollama-chat python=3.11
conda activate ollama-chat
pip install -r requirements.txt
```

#### Ollamaã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# Ollamaã‚µãƒ¼ãƒ“ã‚¹èµ·å‹•
sudo systemctl start ollama

# ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
ollama pull qwen2.5:7b
ollama pull gemma3:4b
```

#### ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•

```bash
# ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆ
streamlit run app/simple_chat.py

# ãƒãƒ«ãƒãƒšãƒ¼ã‚¸ç‰ˆï¼ˆãƒ•ãƒ«æ©Ÿèƒ½ï¼‰
streamlit run app/advanced_chat.py
```

## ğŸ’» ä½¿ã„æ–¹

### åŸºæœ¬çš„ãªä½¿ã„æ–¹

1. ãƒ–ãƒ©ã‚¦ã‚¶ã§ `http://localhost:8501` ã«ã‚¢ã‚¯ã‚»ã‚¹
2. ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é¸æŠ
3. ãƒãƒ£ãƒƒãƒˆæ¬„ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦é€ä¿¡
4. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§å¿œç­”ãŒè¡¨ç¤ºã•ã‚Œã¾ã™

### é«˜åº¦ãªæ©Ÿèƒ½

#### ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®š

```python
# config/characters/custom.json
{
  "name": "ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ",
  "personality": "ä¸å¯§ã§è¦ªåˆ‡",
  "system_prompt": "ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚",
  "temperature": 0.7,
  "top_p": 0.9
}
```

#### ãƒ¢ãƒ‡ãƒ«è¨­å®š

```yaml
# config/models.yaml
models:
  qwen2.5:
    version: "7b"
    context_size: 32768
    gpu_layers: 35
    temperature_default: 0.7
    
  gemma3:
    version: "4b"
    context_size: 8192
    gpu_layers: 28
    temperature_default: 0.8
```

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

### RTX A5000ã§ã®å®Ÿæ¸¬å€¤

| ãƒ¢ãƒ‡ãƒ« | ã‚µã‚¤ã‚º | VRAMä½¿ç”¨é‡ | ãƒˆãƒ¼ã‚¯ãƒ³/ç§’ | åˆå›å¿œç­”æ™‚é–“ |
|--------|--------|------------|-------------|--------------|
| Qwen2.5 | 7B | 5.2GB | 65-80 | 0.8ç§’ |
| Gemma3 | 4B | 2.6GB | 90-120 | 0.4ç§’ |
| Gemma3 | 12B | 7.2GB | 35-50 | 1.2ç§’ |
| GPT-OSS | 20B | 13.8GB | 25-35 | 2.1ç§’ |

## ğŸ”§ ç’°å¢ƒå¤‰æ•°

```bash
# .env ãƒ•ã‚¡ã‚¤ãƒ«
OLLAMA_HOST=localhost:11434
OLLAMA_NUM_PARALLEL=2
OLLAMA_MAX_LOADED_MODELS=3
STREAMLIT_SERVER_PORT=8501
STREAMLIT_THEME=dark
```

## ğŸ“¦ ä¸»ãªä¾å­˜é–¢ä¿‚

- **streamlit**: 1.32.0 - WebUIãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
- **ollama**: 0.3.0 - Ollama Python ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
- **pandas**: ãƒ‡ãƒ¼ã‚¿å‡¦ç†
- **plotly**: ã‚°ãƒ©ãƒ•è¡¨ç¤º
- **pyyaml**: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†

## ğŸ› ï¸ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### Ollamaæ¥ç¶šã‚¨ãƒ©ãƒ¼

```bash
# ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹ç¢ºèª
sudo systemctl status ollama

# å†èµ·å‹•
sudo systemctl restart ollama

# ãƒ­ã‚°ç¢ºèª
journalctl -u ollama -f
```

### GPUèªè­˜å•é¡Œ

```bash
# NVIDIA-SMIç¢ºèª
nvidia-smi

# CUDAç¢ºèª
nvcc --version

# Ollama GPUè¨­å®š
export OLLAMA_GPU_LAYERS=35
```

### ãƒ¡ãƒ¢ãƒªä¸è¶³

```bash
# ã‚¹ãƒ¯ãƒƒãƒ—è¿½åŠ ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
sudo fallocate -l 16G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

## ğŸš§ ä»Šå¾Œã®é–‹ç™ºäºˆå®š

- [ ] éŸ³å£°å…¥å‡ºåŠ›å¯¾å¿œï¼ˆVOICEVOXé€£æºï¼‰
- [ ] ãƒãƒ«ãƒãƒ¦ãƒ¼ã‚¶ãƒ¼å¯¾å¿œ
- [ ] RAGï¼ˆRetrieval Augmented Generationï¼‰å®Ÿè£…
- [ ] ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ©Ÿèƒ½
- [ ] AI-VTuberé€£æºæ©Ÿèƒ½
- [ ] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ«
- [ ] ãƒ¢ãƒã‚¤ãƒ«å¯¾å¿œUI

## ğŸ¤ ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³

ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æ­“è¿ã—ã¾ã™ï¼å¤§ããªå¤‰æ›´ã®å ´åˆã¯ã€ã¾ãšissueã‚’é–‹ã„ã¦å¤‰æ›´å†…å®¹ã‚’è­°è«–ã—ã¦ãã ã•ã„ã€‚

1. ãƒ•ã‚©ãƒ¼ã‚¯
2. ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ–ãƒ©ãƒ³ãƒä½œæˆ (`git checkout -b feature/AmazingFeature`)
3. ã‚³ãƒŸãƒƒãƒˆ (`git commit -m 'Add some AmazingFeature'`)
4. ãƒ—ãƒƒã‚·ãƒ¥ (`git push origin feature/AmazingFeature`)
5. ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆ

## ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚è©³ç´°ã¯[LICENSE](LICENSE)ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## ğŸ™ è¬è¾

- [Ollama](https://ollama.ai/) - ãƒ­ãƒ¼ã‚«ãƒ«LLMå®Ÿè¡Œç’°å¢ƒ
- [Streamlit](https://streamlit.io/) - Pythonã®Webã‚¢ãƒ—ãƒªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
- [Qwen2.5](https://github.com/QwenLM/Qwen2.5) - é«˜æ€§èƒ½è¨€èªãƒ¢ãƒ‡ãƒ«

## ğŸ“§ é€£çµ¡å…ˆ

- Issue: [GitHub Issues](https://github.com/yourusername/ollama-streamlit-chat/issues)
- Discussion: [GitHub Discussions](https://github.com/yourusername/ollama-streamlit-chat/discussions)

---

**Made with â¤ï¸ for Local AI Community**